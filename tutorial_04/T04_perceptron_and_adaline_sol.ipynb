{"nbformat_minor": 1, "cells": [{"source": "This jupyter notebook is prepared by [Chun-Kit Yeung](https://ckyeungac.com)\n\n# Introduction\n**What do we cover in this tutorial?**\n\nIn this tutorial, we will use the Perceptron and ADALINE to tackle the iris classification problem introduced in tutorial 2. However, since the iris dataset has 3 labels while both perceptron and ADALINE are a binary classifier, we will only use the first two classes in the iris dataset.", "cell_type": "markdown", "metadata": {"collapsed": true}}, {"source": "# Preprocess the iris data", "cell_type": "markdown", "metadata": {}}, {"execution_count": 1, "cell_type": "code", "source": "import numpy as np\nfrom sklearn.datasets import load_iris\n\n# load the iris dataset\niris = load_iris()", "outputs": [], "metadata": {}}, {"execution_count": 2, "cell_type": "code", "source": "iris.target[:100] # the first 100 elements are either 0, or 1", "outputs": [{"execution_count": 2, "output_type": "execute_result", "data": {"text/plain": "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1])"}, "metadata": {}}], "metadata": {}}, {"execution_count": 3, "cell_type": "code", "source": "# select the target data (first 100 data points) with slicing\nX_iris = iris.data[:100]\ny_iris = iris.target[:100]", "outputs": [], "metadata": {}}, {"execution_count": 4, "cell_type": "code", "source": "# spliting the data into training set and test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_iris, y_iris, test_size=0.2, random_state=42)", "outputs": [], "metadata": {}}, {"execution_count": 5, "cell_type": "code", "source": "X_train.shape, X_test.shape, y_train.shape, y_test.shape", "outputs": [{"execution_count": 5, "output_type": "execute_result", "data": {"text/plain": "((80, 4), (20, 4), (80,), (20,))"}, "metadata": {}}], "metadata": {}}, {"source": "# Using Perceptron from sklearn\nHere, we first train a perceptron (documentation can be found [here](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html)) provided by scikit-learn.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 6, "cell_type": "code", "source": "# import the Perceptron library from sklearn and train the perceptron\nfrom sklearn.linear_model import Perceptron\n# import the accuracy_score from sklearn\nfrom sklearn.metrics import accuracy_score", "outputs": [], "metadata": {}}, {"execution_count": 7, "cell_type": "code", "source": "# Define and train the perceptron\nclf = Perceptron(fit_intercept=True, eta0=0.5) # eta0 is the learning rate\nclf.fit(X_train, y_train)", "outputs": [{"execution_count": 7, "output_type": "execute_result", "data": {"text/plain": "Perceptron(alpha=0.0001, class_weight=None, eta0=0.5, fit_intercept=True,\n      n_iter=5, n_jobs=1, penalty=None, random_state=0, shuffle=True,\n      verbose=0, warm_start=False)"}, "metadata": {}}], "metadata": {}}, {"execution_count": 8, "cell_type": "code", "source": "y_pred_train = clf.predict(X_train)\naccuracy_score(y_train, y_pred_train)", "outputs": [{"execution_count": 8, "output_type": "execute_result", "data": {"text/plain": "1.0"}, "metadata": {}}], "metadata": {}}, {"execution_count": 9, "cell_type": "code", "source": "y_pred_test = clf.predict(X_test)\naccuracy_score(y_test, y_pred_test)", "outputs": [{"execution_count": 9, "output_type": "execute_result", "data": {"text/plain": "1.0"}, "metadata": {}}], "metadata": {}}, {"source": "# Using ADALINE from sklearn\n**Unfortunately, there is no ADALINE in scikit-learn.** Yet, ADALINE is simply just a special case of artificial neural network with only 1 input layer and 1 output layer. As an altenative, we simply consider **a multi-layer perceptron (MLP) with no hidden layer** to be the ADALINE (even thought they are not exactly the same.)\n\nYet, so as to align with the course materials, I implemented a class called AdalineSGD (available in ''*DIY: Build your own ADALINE with stochastic gradient descent*'') for you to use.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 10, "cell_type": "code", "source": "from sklearn.neural_network import MLPClassifier\nclf = MLPClassifier(\n    hidden_layer_sizes=(), # empty tuple -> no hidden layer used.\n    activation='tanh', # tanh return a value from [-1, 1], similar to step function\n    solver='sgd', # stochastic gradient descent\n)\nclf.fit(X_train, y_train)", "outputs": [{"execution_count": 10, "output_type": "execute_result", "data": {"text/plain": "MLPClassifier(activation='tanh', alpha=0.0001, batch_size='auto', beta_1=0.9,\n       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n       hidden_layer_sizes=(), learning_rate='constant',\n       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n       nesterovs_momentum=True, power_t=0.5, random_state=None,\n       shuffle=True, solver='sgd', tol=0.0001, validation_fraction=0.1,\n       verbose=False, warm_start=False)"}, "metadata": {}}], "metadata": {}}, {"execution_count": 11, "cell_type": "code", "source": "y_pred_train = clf.predict(X_train)\naccuracy_score(y_train, y_pred_train)", "outputs": [{"execution_count": 11, "output_type": "execute_result", "data": {"text/plain": "1.0"}, "metadata": {}}], "metadata": {}}, {"execution_count": 12, "cell_type": "code", "source": "y_pred_test = clf.predict(X_test)\naccuracy_score(y_test, y_pred_test)", "outputs": [{"execution_count": 12, "output_type": "execute_result", "data": {"text/plain": "1.0"}, "metadata": {}}], "metadata": {}}, {"source": "# DIY: Build your own ADALINE with stochastic gradient descent", "cell_type": "markdown", "metadata": {}}, {"execution_count": 13, "cell_type": "code", "source": "from numpy.random import seed\n\nclass AdalineSGD(object):\n    \"\"\"ADAptive LInear NEuron classifier.\n\n    Parameters\n    ------------\n    eta : float\n        Learning rate\n    n_iter : int\n        Number of iteration passes over the training dataset.\n\n    Attributes\n    -----------\n    w_ : 1d-array\n        Weights after fitting.\n    errors_ : list\n        Number of misclassifications in every epoch.\n    shuffle : bool (default: True)\n        Shuffles training data every epoch if True to prevent cycles.\n    random_state : int\n        Set random state for shuffling and initializing the weights.\n        \n    \"\"\"\n    def __init__(self, eta=0.01, n_iter=50, shuffle=True, random_state=1):\n        self.eta = eta\n        self.n_iter = n_iter\n        self.w_initialized = False\n        self.shuffle = shuffle\n        if random_state:\n            seed(random_state)\n            \n    def _shuffle(self, X, y):\n        \"\"\"Shuffle training data\"\"\"\n        r = np.random.permutation(len(y))\n        return X[r], y[r]\n    \n    def _initialize_weights(self, m):\n        \"\"\"Randomly initialize weights\"\"\"\n        self.w_ = np.random.normal(loc=0.0, scale=0.01, size=1 + m)\n        self.w_initialized = True\n            \n    def _update_weights(self, xi, target):\n        \"\"\"Apply Adaline learning rule to update the weights for a single sample.\"\"\"\n        output = self.activation(xi)\n        error = (target - output)\n        self.w_[1:] += self.eta * xi.dot(error)\n        self.w_[0] += self.eta * error\n        cost = 0.5 * error**2\n        return cost\n    \n    def net_input(self, X):\n        \"\"\"Calculate net input\"\"\"\n        return np.dot(X, self.w_[1:]) + self.w_[0] \n\n    def activation(self, X):\n        \"\"\"Compute linear activation\"\"\"\n        return self.net_input(X)\n    \n    def predict(self, X):\n        \"\"\"Return class label after unit step\"\"\"\n        return np.where(self.activation(X) >= 0.0, 1, 0)\n        \n    def fit(self, X, y):\n        \"\"\" Fit training data.\n\n        Parameters\n        ----------\n        X : {array-like}, shape = [n_samples, n_features]\n            Training vectors, where n_samples is the number of samples and\n            n_features is the number of features.\n        y : array-like, shape = [n_samples]\n            Target values.\n\n        Returns\n        -------\n        self : object\n\n        \"\"\"\n        # initialize the weight\n        self._initialize_weights(X.shape[1])\n        \n        # make a copy of y: y_\n        y_ = np.copy(y) # e.g. it is np.array([1,1,0,1,0,0,1])\n        \n        #convert the value of 0 to -1 in y_\n        y_[y_==0] = -1 # e.g. it is now np.array([1,1,-1,1,-1,-1,1])\n        \n        for i in range(self.n_iter):\n            # shuffle the data if required\n            if self.shuffle:\n                X, y_ = self._shuffle(X, y_)\n            \n            # update the weight sample by sample\n            for xi, target in zip(X, y_):\n                self._update_weights(xi, target)\n            \n        return self", "outputs": [], "metadata": {}}, {"execution_count": 14, "cell_type": "code", "source": "clf = AdalineSGD()\nclf.fit(X_train, y_train)", "outputs": [{"execution_count": 14, "output_type": "execute_result", "data": {"text/plain": "<__main__.AdalineSGD at 0x7f4764a99320>"}, "metadata": {}}], "metadata": {}}, {"execution_count": 15, "cell_type": "code", "source": "y_pred_train = clf.predict(X_train)\naccuracy_score(y_train, y_pred_train)", "outputs": [{"execution_count": 15, "output_type": "execute_result", "data": {"text/plain": "1.0"}, "metadata": {}}], "metadata": {}}, {"execution_count": 16, "cell_type": "code", "source": "y_pred_test = clf.predict(X_test)\naccuracy_score(y_test, y_pred_test)", "outputs": [{"execution_count": 16, "output_type": "execute_result", "data": {"text/plain": "1.0"}, "metadata": {}}], "metadata": {}}, {"execution_count": 17, "cell_type": "code", "source": "# see what is the learnt weight\nclf.w_", "outputs": [{"execution_count": 17, "output_type": "execute_result", "data": {"text/plain": "array([-0.11398174, -0.12280366, -0.31600092,  0.50167422,  0.40759171])"}, "metadata": {}}], "metadata": {}}, {"source": "# DIY: Build your own Preceptron", "cell_type": "markdown", "metadata": {}}, {"execution_count": 18, "cell_type": "code", "source": "class Perceptron2(object):\n    \"\"\"Perceptron classifier.\n    \n    Parameters\n    ------------\n    eta: float \n        Learning rate (between 0.0 and 1.0)\n    n_iter: int\n        Number of epochs, i.e. number of iteration passes over the training dataset.\n        \n    Attributes\n    ------------\n    w_: 1d-array\n        Weights after fitting.\n    errors_: list\n        Number of misclassifications in every epoch.\n    random_state : int\n        The seed of the pseudo random number generator.\n    \"\"\"\n    \n    def __init__(self, eta=0.01, n_iter=100, random_state=1):\n        self.eta = eta\n        self.n_iter = n_iter\n        self.w_initialized = False\n        self.random_state = random_state\n        if random_state:\n            seed(random_state)\n            \n    def _initialize_weights(self, m):\n        \"\"\"Randomly initialize weights\"\"\"\n        self.w_ = np.random.normal(loc=0.0, scale=0.01, size=1 + m)\n        self.w_initialized = True\n    \n    def net_input(self, X):\n        \"\"\"Calculate net input\"\"\"\n        return np.dot(X, self.w_[1:]) + self.w_[0]\n    \n    def _predict(self, X):\n        \"\"\"Return class label after unit step for internal usage.\"\"\"\n        return np.where(self.net_input(X) >= 0.0, 1, -1)\n    \n    def predict(self, X):\n        \"\"\"Return class label after unit step for external usage.\"\"\"\n        return np.where(self.net_input(X) >= 0.0, 1, 0)\n                \n    def _update_weights(self, xi, target):\n        \"\"\"Apply Perceptron learning rule to update the weights for a single sample.\"\"\"\n        output = self._predict(xi)\n        self.w_[1:] += self.eta * xi * (target - output)\n        self.w_[0] += self.eta * (target - output)\n    \n    def fit(self, X, y):\n        \"\"\"Fit training data.\n\n        Parameters\n        ----------\n        X : {array-like}, shape = [n_samples, n_features]\n            Training vectors, where n_samples is the number of samples and\n            n_features is the number of features.\n        y : array-like, shape = [n_samples]\n            Target values.\n\n        Returns\n        -------\n        self : object\n\n        \"\"\"\n        # initialize the weight\n        self._initialize_weights(X.shape[1])\n        \n        # make a copy of y: y_\n        y_ = np.copy(y)\n        \n        #convert the value of 0 to -1 in y_\n        y_[y_==0] = -1\n        \n        for _ in range(self.n_iter):\n            for xi, target in zip(X, y_):\n                self._update_weights(xi, target)\n        return self", "outputs": [], "metadata": {}}, {"execution_count": 19, "cell_type": "code", "source": "clf = Perceptron2()\nclf.fit(X_train, y_train)", "outputs": [{"execution_count": 19, "output_type": "execute_result", "data": {"text/plain": "<__main__.Perceptron2 at 0x7f4764a99e48>"}, "metadata": {}}], "metadata": {}}, {"execution_count": 20, "cell_type": "code", "source": "y_pred_train = clf.predict(X_train)\naccuracy_score(y_train, y_pred_train)", "outputs": [{"execution_count": 20, "output_type": "execute_result", "data": {"text/plain": "1.0"}, "metadata": {}}], "metadata": {}}, {"execution_count": 21, "cell_type": "code", "source": "y_pred_test = clf.predict(X_test)\naccuracy_score(y_test, y_pred_test)", "outputs": [{"execution_count": 21, "output_type": "execute_result", "data": {"text/plain": "1.0"}, "metadata": {}}], "metadata": {}}, {"execution_count": 22, "cell_type": "code", "source": "# see what is the learnt weight\nclf.w_", "outputs": [{"execution_count": 22, "output_type": "execute_result", "data": {"text/plain": "array([-0.00375655, -0.03611756, -0.14128172,  0.18927031,  0.08665408])"}, "metadata": {}}], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "3.4.5", "name": "python", "file_extension": ".py", "pygments_lexer": "ipython3", "codemirror_mode": {"version": 3, "name": "ipython"}}}}