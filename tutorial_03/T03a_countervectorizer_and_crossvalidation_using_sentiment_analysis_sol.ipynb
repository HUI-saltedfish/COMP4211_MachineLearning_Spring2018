{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This jupyter notebook is prepared by [Chun-Kit Yeung](https://ckyeungac.com)\n",
    "\n",
    "# Introduction\n",
    "**What did we cover in the last tutorial?**\n",
    "\n",
    "In the last tutorial, we have gone through the following codes to using the naive Bayes classifier in scikit-learn and evaluating the model performance, training accuracy and test accuracy.\n",
    "\n",
    "**What do we cover in this tutorial?**\n",
    "\n",
    "In this tutorial, I will introduce the python packages that are required to complete assignment 1, including\n",
    "+ extract feature from text: `sklearn.feature_extraction.test.CountVectorizer`\n",
    "+ calculating the accuracy score: `sklearn.metrics.accuracy_score`\n",
    "+ performing the 5-fold cross-validation: `sklearn.cross_validation.KFold`\n",
    "\n",
    "As for the `KFold`, I will only cover what it does, but not the exact usage of `KFold` in machine learning. You have to work on your own as it is one of task in the assignment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy version: 1.14.1\n",
      "scikit-learn version: 0.19.1\n",
      "matplotlib version: 2.1.2\n"
     ]
    }
   ],
   "source": [
    "# import the required packages\n",
    "import numpy as np\n",
    "import sklearn \n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# tell the jupyter notebook that plot the diagram directly in the output\n",
    "%matplotlib inline \n",
    "    \n",
    "print ('numpy version:', np.__version__)\n",
    "print ('scikit-learn version:', sklearn.__version__)\n",
    "print ('matplotlib version:', matplotlib.__version__)\n",
    "\n",
    "SEP = '='*30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a toy dataset that include a list of tuples `(<sentence>, <sentiment_label>)` to have a taste in handling the text data.\n",
    "\n",
    "`<sentence>` is a english sentence. Usually, it contains some of the positive/negative information/emotion. So, one of the popular task in machine learning is sentiment analysis in which machine learinng experts build models to predict the emotion of a sentence. In this toy example, the `<sentiment>` label is simply 0 or 1, indicating a negative or positive emotion is observed in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# (<sentence>, <sentiment_label>)\n",
    "# sentiment_label=1 means positive, 0 means negative.\n",
    "toy_sentiment_dataset = [\n",
    "    ('Today I am so happy and you are so happy.', 1),\n",
    "    ('Happy is so important and you are so important too', 1),\n",
    "    ('you look so great and happy!', 1),\n",
    "    ('Don\\'t be so sad, dude', 0),\n",
    "    ('Machine learning is difficult', 0),\n",
    "    ('COMP 4211 is the goodest', 1),\n",
    "    ('James is so good', 1),\n",
    "    ('Kit is so sad', 0),\n",
    "]\n",
    "\n",
    "sentences = [data[0] for data in toy_sentiment_dataset]\n",
    "sentiment = [data[1] for data in toy_sentiment_dataset]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert text to vector using [`CountVectorizer`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)**\n",
    "\n",
    "It is a handy package to convert a collection of text documents to a matrix of token counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(stop_words='english') # stop words are the word that contains no information\n",
    "# vecotrizer.fit_transform(sentences)\n",
    "vectorizer.fit(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# See what is it look like in vector representation\n",
    "sample = ['COMP 4211 is the goodest.']\n",
    "sample_vector = vectorizer.transform(sample).toarray() \n",
    "print(sample_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array(['4211', 'comp', 'goodest'], dtype='<U9')]\n"
     ]
    }
   ],
   "source": [
    "# See what is the content if we convert the vector back to words. (Not sentence!)\n",
    "print(vectorizer.inverse_transform(sample_vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Building toy sentiment prediction model using [`artificial neural network`](http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ckyeungac/anaconda/envs/dlenv/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(10,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier(hidden_layer_sizes=(10,))\n",
    "clf.fit(vectorizer.transform(sentences), sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluating the artificial neural network using the [`accuracy_score`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) provided by scikit-learn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = clf.predict(vectorizer.transform(sentences))\n",
    "acc = accuracy_score(sentiment, y_pred)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Fold Cross-validation\n",
    "In k-fold cross-validation, the original sample is randomly partitioned into k equal sized subsamples, so-called **folds**. Of the k folds, **a single fold is retained as the test set**, and **the remaining k â 1 subsamples are used as training set**. The cross-validation process is then repeated k times (the folds), with each of the k subsamples used exactly once as the validation data.\n",
    "\n",
    "In scikit-learn, it provides a handy package called [`KFold`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html) to facilitate k-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "==============================\n",
      "Training index: [2 3 4 5 6 7 8 9]\n",
      "Test index: [0 1]\n",
      "============================== \n",
      "\n",
      "Fold 2\n",
      "==============================\n",
      "Training index: [0 1 4 5 6 7 8 9]\n",
      "Test index: [2 3]\n",
      "============================== \n",
      "\n",
      "Fold 3\n",
      "==============================\n",
      "Training index: [0 1 2 3 6 7 8 9]\n",
      "Test index: [4 5]\n",
      "============================== \n",
      "\n",
      "Fold 4\n",
      "==============================\n",
      "Training index: [0 1 2 3 4 5 8 9]\n",
      "Test index: [6 7]\n",
      "============================== \n",
      "\n",
      "Fold 5\n",
      "==============================\n",
      "Training index: [0 1 2 3 4 5 6 7]\n",
      "Test index: [8 9]\n",
      "============================== \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=5, shuffle=False) # without shuffling the data\n",
    "some_2d_array = np.arange(20).reshape((10, 2))\n",
    "fold_count = 0\n",
    "for train_idx, test_idx in kf.split(some_2d_array):\n",
    "    fold_count += 1\n",
    "    print(\"Fold\", fold_count)\n",
    "    print(SEP)\n",
    "    print(\"Training index:\", train_idx)\n",
    "    print(\"Test index:\", test_idx)\n",
    "    print(SEP, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "==============================\n",
      "Training index: [0 2 3 4 5 6 7 9]\n",
      "Test index: [1 8]\n",
      "============================== \n",
      "\n",
      "Fold 2\n",
      "==============================\n",
      "Training index: [1 2 3 4 6 7 8 9]\n",
      "Test index: [0 5]\n",
      "============================== \n",
      "\n",
      "Fold 3\n",
      "==============================\n",
      "Training index: [0 1 3 4 5 6 8 9]\n",
      "Test index: [2 7]\n",
      "============================== \n",
      "\n",
      "Fold 4\n",
      "==============================\n",
      "Training index: [0 1 2 3 5 6 7 8]\n",
      "Test index: [4 9]\n",
      "============================== \n",
      "\n",
      "Fold 5\n",
      "==============================\n",
      "Training index: [0 1 2 4 5 7 8 9]\n",
      "Test index: [3 6]\n",
      "============================== \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42) # with shuffling the data\n",
    "some_2d_array = np.arange(20).reshape((10, 2))\n",
    "fold_count = 0\n",
    "for train_idx, test_idx in kf.split(some_2d_array):\n",
    "    fold_count += 1\n",
    "    print(\"Fold\", fold_count)\n",
    "    print(SEP)\n",
    "    print(\"Training index:\", train_idx)\n",
    "    print(\"Test index:\", test_idx)\n",
    "    print(SEP, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original some_2d_array:\n",
      " [[ 0  1]\n",
      " [ 2  3]\n",
      " [ 4  5]\n",
      " [ 6  7]\n",
      " [ 8  9]\n",
      " [10 11]\n",
      " [12 13]\n",
      " [14 15]\n",
      " [16 17]\n",
      " [18 19]]\n",
      "==============================\n",
      "sliced some_2d_array[train_idx]:\n",
      " [[ 0  1]\n",
      " [ 2  3]\n",
      " [ 4  5]\n",
      " [ 6  7]\n",
      " [ 8  9]\n",
      " [10 11]\n",
      " [12 13]\n",
      " [14 15]]\n"
     ]
    }
   ],
   "source": [
    "# let say the train index is \n",
    "train_idx = [0, 1, 2, 3, 4, 5, 6, 7]\n",
    "\n",
    "# you can assess the training data by doing a slicing/indexing\n",
    "print(\"original some_2d_array:\\n\", some_2d_array)\n",
    "print(SEP)\n",
    "print(\"sliced some_2d_array[train_idx]:\\n\", some_2d_array[train_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
